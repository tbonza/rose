{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this notebook to write your code for assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import string\n",
    "import time\n",
    "import unidecode\n",
    "import random\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters\n",
    "\n",
    "Scavenger hunts are fun but I like putting my hyperparameters in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALL_CHARACTERS = ' abcdefghijklmnopqrstuvwxyz'\n",
    "ALL_CHARACTERS = string.printable\n",
    "HP = {\n",
    "    \n",
    "    # Data pre-processing\n",
    "    \n",
    "    \"all_chars\": ALL_CHARACTERS,\n",
    "    \"char_set\": set(ALL_CHARACTERS),\n",
    "    \"random_seed\": 42,\n",
    "    \"n_chars\": 0, # will need to update\n",
    "    \"chunk_len\": 40,\n",
    "    \n",
    "    # Training model\n",
    "    \n",
    "    \"n_epochs\": 2000,\n",
    "    \"hidden_size\": 100,\n",
    "    \"n_layers\": 1,\n",
    "    \"lr\": 0.005,\n",
    "    \"temperature\": 0.8,\n",
    "    \"prime_str\": \"a\",\n",
    "    \"predict_len\": 100,\n",
    "    \n",
    "    # Model reporting\n",
    "    \n",
    "    \"print_every\": 100,\n",
    "    \"plot_every\": 10,\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing data\n",
    "\n",
    "Read the data file of `shakespeare.txt` and prepare the inputs/outputs to your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    chars = list(HP[\"char_set\"])\n",
    "    lines = []\n",
    "    for line in data:\n",
    "        line = ''.join([i for i in line.lower().strip() if i in chars])\n",
    "        if len(line) > 0:\n",
    "            lines.append(line)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_chunk():\n",
    "    random.seed(HP[\"random_seed\"])\n",
    "    start_index = random.randint(0, HP[\"n_chars\"] - HP[\"chunk_len\"])\n",
    "    end_index = start_index + HP[\"chunk_len\"] + 1\n",
    "    return FILE[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn string into list of longs\n",
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(len(string)).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[c] = HP[\"all_chars\"].index(string[c])\n",
    "    return Variable(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_training_set():    \n",
    "    chunk = random_chunk()\n",
    "    inp = char_tensor(chunk[:-1])\n",
    "    target = char_tensor(chunk[1:])\n",
    "    return inp, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data ingest\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/shakespeare.txt\", \"r\") as infile:\n",
    "    data = infile.readlines()\n",
    "    \n",
    "data = clean_data(data)\n",
    "FILE = ''.join(data)\n",
    "HP['n_chars'] = len(FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 91873 total characters and 100 unique characters in your data.\n",
      "\n",
      "Example file chunk: '1from fairest creatures we desire increa'\n"
     ]
    }
   ],
   "source": [
    "print('There are %d total characters and %d unique characters in your data.' % (HP['n_chars'], len(HP['char_set'])))\n",
    "\n",
    "print(\"\\nExample file chunk: '{}'\".format(FILE[:HP['chunk_len']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random chunk:\n",
      "of view is pleased to dote.nor are mine c\n",
      "\n",
      "Transforms to char tensor:\n",
      "tensor([24, 15, 94, 31, 18, 14, 32, 94, 18, 28, 94, 25, 21, 14, 10, 28, 14, 13,\n",
      "        94, 29, 24, 94, 13, 24, 29, 14, 75, 23, 24, 27, 94, 10, 27, 14, 94, 22,\n",
      "        18, 23, 14, 94, 12])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rchunk = random_chunk()\n",
    "rutens = char_tensor(rchunk)\n",
    "\n",
    "print(\"Random chunk:\\n{}\\n\\nTransforms to char tensor:\\n{}\\n\".format(rchunk, rutens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Model\n",
    "\n",
    "Implement a character-based LSTM model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        input = self.encoder(input.view(1, -1))\n",
    "        output, hidden = self.gru(input.view(1, 1, -1), hidden)\n",
    "        output = self.decoder(output.view(1, -1))\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(self.n_layers, 1, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(inp, target):\n",
    "    hidden = decoder.init_hidden()\n",
    "    decoder.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    for c in range(HP['chunk_len']):\n",
    "        output, hidden = decoder(inp[c], hidden)\n",
    "        label = target[c]\n",
    "        label = label.unsqueeze(0)\n",
    "        loss += criterion(output, label)\n",
    "\n",
    "    loss.backward()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data.item() / HP['chunk_len']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(prime_str='a', predict_len=100, temperature=0.8):\n",
    "    hidden = decoder.init_hidden()\n",
    "    prime_input = char_tensor(prime_str)\n",
    "    predicted = prime_str\n",
    "\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = decoder(prime_input[p], hidden)\n",
    "    inp = prime_input[-1]\n",
    "    \n",
    "    for p in range(predict_len):\n",
    "        output, hidden = decoder(inp, hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1).item()\n",
    "        \n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = HP['all_chars'][top_i]\n",
    "        predicted += predicted_char\n",
    "        inp = char_tensor(predicted_char)\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main training routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3m 3s (100 5%) 0.0594]\n",
      "whe ce.nor are mine co dote.nor are mine ciew is pleeased to dote.note.nor are mine co dote.nor are ar \n",
      "\n",
      "[6m 7s (200 10%) 0.0139]\n",
      "whe co dote.nor are mine co dote.nor are mine cte.nor are mine co dote.nor are mine co dote.nor are mi \n",
      "\n",
      "[9m 11s (300 15%) 0.0069]\n",
      "whe co dote.nor are mine co dote.nor are mine co dote.nor are mine co dote.nor are mine co dote.nor ar \n",
      "\n",
      "[47m 57s (400 20%) 0.0043]\n",
      "whe co dote.nor are mine co dote.nor are mine co dote.nor are mine csed to dote.nor are mine ce co dot \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "decoder = RNN(HP['n_chars'], HP['hidden_size'], HP['n_chars'], HP['n_layers'])\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=HP['lr'])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0\n",
    "\n",
    "for epoch in range(1, HP['n_epochs'] + 1):\n",
    "    loss = train(*random_training_set())       \n",
    "    loss_avg += loss\n",
    "\n",
    "    if epoch % HP['print_every'] == 0:\n",
    "        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / HP['n_epochs'] * 100, loss))\n",
    "        print(evaluate('wh', HP[\"predict_len\"]), '\\n')\n",
    "\n",
    "    if epoch % HP['plot_every'] == 0:\n",
    "        all_losses.append(loss_avg / HP['plot_every'])\n",
    "        loss_avg = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the Training Losses\n",
    "\n",
    "Plotting the historical loss during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating at different \"temperatures\"\n",
    "\n",
    "Changing the `temperature` argument (variance) for 1.5, 0.75. 0.25 and sample outputs from the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
