\documentclass{beamer}
 
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
%Information to be included in the title page:
\title{Semi-supervised Learning with Deep Generative Models}
\subtitle{Kingma et. al. (2014)}
\author{Tyler Brown}
\institute{CS 7180}
\date{}
 
\begin{document}
 
\frame{\titlepage}
 
\begin{frame}
  \frametitle{Motivating Question}
  How can we model data of increasing size when obtaining label 
  information is difficult?
\end{frame}

\begin{frame}
  \frametitle{High-level Answer}

  We can estimate missing label information by
  using a probabilistic model.
  
\end{frame}

\begin{frame}
  \frametitle{Specifying the Probabilistic Model for Missing Labels}

    \begin{itemize}
  \item Data appears as pairs $(\mathbf{X}, \mathbf{Y}) =
    \{(\mathbf{x}_1, y_1), ..., (\mathbf{x}_N, y_N)\}$
  with the $i$-th observation $x_i \in \mathbb{R}^D$ and a
  corresponding class label $y_i \in \{1, ..., L\}$

  \begin{itemize}
\item Each pair of observations $(x_i,y_i)$ has a corresponding
  latent variable $z_i$

\item Empirical distribution over the labelled and unabelled subsets
  is referred to as $\tilde{p_l}(\mathbf{x}, y)$ and
  $\tilde{p_u}(\mathbf{x})$
  \end{itemize}
\item We can estimate $y_i$ for $x_i$ in distribution
  $\tilde{p_u}(\mathbf{x})$ by finding the maximum
  probability of $p(y_i)$ by using a set of features
  related to $z_i$ and a predictive model
  \begin{enumerate}
  \item \textbf{Latent-feature discriminative model (M1)}
  \item \textbf{Generative semi-supervised model (M2)}
  \item \textbf{Stacked generative semi-supervised model (M1+M2)}
  \end{enumerate}
  \end{itemize}
  \end{frame}

\begin{frame}
  \frametitle{Bayes Rule is used when specifying M1 \& M2}

  \begin{align*}
    p(x,y) &= p(x)p(y|x) \\
    &= p(y)p(x|y) \\
    p(x|y) &= \frac{p(x)p(y|x)}{p(y)}
  \end{align*}

  for models M1 \footnotemark, $p(z|x)$, and M2 \footnotemark; $p(y|x)$

  \footnotetext[1]{Kingma et. al. (2014) equation (1)}
  \footnotetext[2]{Kingma et. al. (2014) equation (2)}
  
\end{frame}

\begin{frame}
  \frametitle{(M1) Latent-feature discriminative model}

  \[
  y \Leftarrow   p(z|x) = \frac{p(z)p(x|z)}{p(x)}
  \]

  where

  \begin{align*}
    p(z) &= \mathcal{N}(z|0,I) \qquad \text{Gaussian distribution of $z$ given a missing label $y$}\\
    p(x|z) &= f(x;z,\theta) \qquad \text{likelihood function, parameters
      $\theta$ of a set of $z$} \\
    p(x) &= \tilde{p_u}(x) \qquad \text{unlabelled subset of $x_i \in \mathbb{R}^D$}
    \end{align*}

  Kingma et. al. (2014) eq. (1)
\end{frame}

\begin{frame}
  \frametitle{(M1) Predicting Class Labels $y$}

  Approximate samples from the posterior distribution over the
  latent variables $p(z|x)$ are used as features to train a
  classifier that predicts class labels $y$

  \begin{itemize}
  \item (transductive) SVM
  \item multinomial regression
  \end{itemize}

  \textbf{TODO:} Add pictures or simulation here
  
\end{frame}

\begin{frame}
  \frametitle{(M2) Generative semi-supervised model}

  \[
  p(y|\mathbf{x}) = \frac{p(\mathbf{x}|y) p(y)}{p(\mathbf{x})} \approx
  \frac{p_\theta (\mathbf{x}|y, \mathbf{z}) p(y)}{p(\mathbf{x})}
  \]

  where

  \begin{align*}
    p(y) &= Cat(y|\mathbf{\pi}) \qquad \text{multinomial distribution, $y$ can be latent} \\
    p(\mathbf{z}) &= \mathcal{N}(\mathbf{z|0,I}) \qquad
    \text{Gaussian distribution of $z$ when missing $y$} \\
    p_\theta (\mathbf{x}|y,z) &= f(\mathbf{x};y,\mathbf{z},\mathbf{\theta})
    \qquad \text{likelihood function, nonlinear parameters} \\
    p(x) & \qquad \qquad \qquad \qquad \text{all $x$ in dist. of real numbers;  $x \in \mathbb{R}^{D}$}
    \end{align*}
  
\end{frame}

\begin{frame}
  \frametitle{Stacked generative semi-supervised model (M1 + M2)}

  Combine M1 and M2
  \begin{enumerate}
  \item Learn a new latent representation $z_1$ from M1
  \item Use embeddings from $z_1$ instead of raw data $x$, to create
    a generative semi-supervised model M2
    \end{enumerate}

  \textbf{TODO:} Add a picture or something here
  \end{frame}
 
\end{document}
